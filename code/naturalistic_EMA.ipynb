{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naturalistic EMA validation\n",
    "\n",
    "Applying the findings from the 4-state correlation work (EMA x UPDRS) onto real-life EMA data.\n",
    "\n",
    "Goals:\n",
    "- analyse real-life variation of EMA values\n",
    "    - inter-individual variation\n",
    "    - intra-individual variation, daily fluctuations, differences between days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Import packages\n",
    "\n",
    "- document versions for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import json\n",
    "import importlib\n",
    "from itertools import product, compress\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from scipy.signal import welch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Python sys', sys.version)\n",
    "print('pandas', pd.__version__)\n",
    "print('numpy', np.__version__)\n",
    "# print('mne_bids', mne_bids.__version__)\n",
    "# print('mne', mne.__version__)\n",
    "# print('sci-py', scipy.__version__)\n",
    "# print('sci-kit learn', sk.__version__)\n",
    "# print('matplotlib', plt_version)\n",
    "\n",
    "\"\"\"\n",
    "Python sys 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]\n",
    "pandas 2.1.1\n",
    "numpy 1.26.0\n",
    "\n",
    "from 16.09\n",
    "\n",
    "Python sys 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]\n",
    "pandas 2.3.2\n",
    "numpy 2.3.3\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import custom functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dbs_home\n",
    "\n",
    "# from dbs_home repo\n",
    "from dbs_home.load_raw.main_load_raw import loadSubject \n",
    "import dbs_home.utils.helpers as home_helpers\n",
    "import dbs_home.utils.ema_utils as home_ema_utils\n",
    "import dbs_home.plot_data.plot_compliance as plot_home_compl\n",
    "import dbs_home.preprocessing.preparing_ema as home_ema_prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from current repo\n",
    "from utils import load_utils, load_data, prep_data\n",
    "from plotting import plot_help\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Home-Data\n",
    "\n",
    "Use pre-operative sessions\n",
    "\n",
    "- use 9-point-converter\n",
    "- use direct. inverter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import EMA home data from raw files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MOMENTS = ['pre-op', 'pre 3MFU', 'post 3MFU']\n",
    "\n",
    "sub_skip = [] # ['hm25',]  # skip full subject\n",
    "# skip per session\n",
    "ses_skip = [['hm20', 'ses03'],]\n",
    "# ses_skip = [['hm14', 'ses03']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions_include = {m: {} for m in MOMENTS}\n",
    "\n",
    "for rec_moment in MOMENTS:\n",
    "\n",
    "    sel_info = home_helpers.select_sessions(target_session=rec_moment)\n",
    "    sel_info = sel_info.set_index(sel_info['study_id'],)\n",
    "    sel_sessions = {sub: ses for sub, ses in sel_info[['study_id', 'Session']].values}\n",
    "\n",
    "    for key, val in sel_sessions.items():\n",
    "        sessions_include[rec_moment][key] = val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sessions_include.keys())\n",
    "print(sessions_include)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data\n",
    "\n",
    "dev for EMA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(home_ema_utils)\n",
    "importlib.reload(load_data)\n",
    "importlib.reload(prep_data)\n",
    "importlib.reload(home_ema_prep)\n",
    "\n",
    "# load all combined data\n",
    "\n",
    "# SUBS_INCL = ['hm14']\n",
    "\n",
    "\n",
    "data = {m: {} for m in MOMENTS}\n",
    "\n",
    "for rec_moment, sub_sess in sessions_include.items():\n",
    "    # rec_moment contains 'pre-op', or 'pre 3MFU', 'post 3MFU', etc\n",
    "    for sub, ses in sub_sess.items():\n",
    "\n",
    "        if sub in sub_skip: continue\n",
    "                    \n",
    "        if [sub, ses] in ses_skip: continue\n",
    "                \n",
    "        ses_class = loadSubject(\n",
    "            sub=sub,\n",
    "            ses=ses,\n",
    "            incl_EMA=True,\n",
    "            incl_ACC=False,\n",
    "        )\n",
    "        temp_df = home_ema_utils.load_ema_df(sub_ses_class=ses_class)\n",
    "        # prepare\n",
    "        temp_df = home_ema_prep.prepare_ema_df(temp_df, ADD_MEANMOVE=True, INVERT_NEG_ITEMS=False,)\n",
    "        ### TODO: CHECK WHY NOT ALL SUBS ARE INVERTED PROPERLY\n",
    "\n",
    "        data[rec_moment][sub] = temp_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Explore naturalistic EMAs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess EMA\n",
    "\n",
    "- merge scores\n",
    "- invert negative-items (higher = clinically better)\n",
    "- mean-correct EMA\n",
    "    - test different normalizations:\n",
    "        - normalize with grand-mean per sub\n",
    "        - normalize with session mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(prep_data)\n",
    "\n",
    "allsubs = []\n",
    "for mom in list(data.keys()): allsubs.extend(list(data[mom].keys()))\n",
    "allsubs = np.unique(allsubs)\n",
    "\n",
    "\n",
    "corr_data = {m: {} for m in MOMENTS}\n",
    "\n",
    "for sub in allsubs:\n",
    "\n",
    "    subdf = home_ema_prep.merge_sub_ema_df(datadict=data, sub=sub)\n",
    "    subdf = home_ema_prep.mean_correct_ema_df(subdf)\n",
    "\n",
    "    # split and palce back as moment dfs    \n",
    "    for moment in MOMENTS:\n",
    "        corr_data[moment][sub] = subdf[subdf['moment'] == moment].reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize first EMA results full group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLOT_ITEMS = ['move_mean', 'walking', 'tremor']\n",
    "\n",
    "PLOT_CORR = False\n",
    "if PLOT_CORR: PLOT_DATADICT = corr_data\n",
    "else: PLOT_DATADICT = data\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(9, 6))\n",
    "fname = 'motorItems_abs_perSub_perSes_1104'\n",
    "if PLOT_CORR: fname = fname.replace('abs', 'corr')\n",
    "\n",
    "x_margin = 2\n",
    "bin_w = 0.5\n",
    "\n",
    "fsize=14\n",
    "\n",
    "x_starts = {list(data.keys())[0]: 0}  # first moment starts at 0\n",
    "\n",
    "subcolors = plot_help.get_sub_colors(PLOT_DATADICT)\n",
    "\n",
    "\n",
    "for i_ax, col in enumerate(PLOT_ITEMS):\n",
    "\n",
    "    for i_mom, moment in enumerate(PLOT_DATADICT.keys()):\n",
    "\n",
    "        if PLOT_CORR and i_mom == 0: col = f'{col}_corr'\n",
    "\n",
    "        # loop over sub-dfs within moment and add specific column values\n",
    "        list_values = [tempdf[col].values for tempdf in PLOT_DATADICT[moment].values()]\n",
    "        box_subs = list(PLOT_DATADICT[moment].keys())  # subs included in this boxplot\n",
    "        # sort by sub id\n",
    "        i_sort = np.argsort(box_subs)\n",
    "        box_subs = [box_subs[i] for i in i_sort]\n",
    "        list_values = [list_values[i] for i in i_sort]\n",
    "\n",
    "        # drop NaN values in lists\n",
    "        list_values = [[v for v in l if not np.isnan(v)] for l in list_values]\n",
    "\n",
    "        # plot boxes for one moment\n",
    "        bp = axes[i_ax].boxplot(list_values, widths=bin_w,\n",
    "                           positions=x_starts[moment] + bin_w * np.arange(len(list_values)),\n",
    "                           patch_artist=True,)\n",
    "        if i_ax == 0:\n",
    "            if moment != list(PLOT_DATADICT.keys())[-1]:\n",
    "                x_starts[list(PLOT_DATADICT.keys())[i_mom + 1]] = x_starts[moment] + len(list_values) * bin_w + x_margin\n",
    "\n",
    "        # Loop over boxes\n",
    "        for patch, patchsub in zip(bp['boxes'], box_subs):\n",
    "            patch.set_facecolor(subcolors[patchsub])\n",
    "\n",
    "    # pretty plot\n",
    "    axes[i_ax].set_ylabel(f'{col}\\n(EMA answer)', size=fsize,)\n",
    "\n",
    "# pretty axes\n",
    "for ax in axes:\n",
    "    if not PLOT_CORR:\n",
    "        ax.set_ylim(0, 10)\n",
    "        ax.set_yticks(np.arange(1, 10, 2))\n",
    "        ax.set_yticklabels(np.arange(1, 10, 2))\n",
    "    else:\n",
    "        ax.set_ylim(-5, 5)\n",
    "        ax.set_yticks(np.arange(-4, 6, 2))\n",
    "        ax.set_yticklabels(np.arange(-4, 6, 2))\n",
    "\n",
    "    ax.set_xticks(list(x_starts.values()))\n",
    "    ax.set_xticklabels(list(x_starts.keys()))\n",
    "    ax.spines[['right', 'top']].set_visible(False)\n",
    "    ax.tick_params(axis='both', labelsize=fsize, size=fsize,)\n",
    "\n",
    "    ax.axhline(0, xmin=0, xmax=1,\n",
    "               color='gray', alpha=.3, zorder=0,)\n",
    "    if PLOT_CORR: ylines = [-4, -2, 2, 4,]\n",
    "    else: ylines = [1, 3, 5, 7, 9]\n",
    "    for yline in ylines:\n",
    "        ax.axhline(yline, xmin=0, xmax=1, color='gray', alpha=.15, zorder=0,)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# plt.savefig(os.path.join(load_utils.get_onedrive_path('figures'),\n",
    "#              'ema_naturalistic', fname),\n",
    "#              dpi=300, facecolor='w',)\n",
    "\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check completion rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for rec_moment in data.keys():\n",
    "\n",
    "    for sub in data[rec_moment].keys():\n",
    "\n",
    "        df = data[rec_moment][sub]\n",
    "\n",
    "        df['Submission'] = pd.to_numeric(df['Submission'], errors='coerce')\n",
    "        rate = df['Submission'].mean()\n",
    "        print(f\"{sub} completion rate @ {rec_moment}: {rate:.0%}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sub-analysis paper: EMA pre-post vs UPDRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.regression.mixed_linear_model import MixedLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_SEL = 'general movement'\n",
    "\n",
    "PER1 = 'pre-op'\n",
    "PER2 = 'post 3MFU'\n",
    "\n",
    "pt_pre = list(data[PER1].keys())\n",
    "pt_sel = [p for p in list(data[PER2].keys()) if p in pt_pre]\n",
    "\n",
    "stat_df = {'sub': [], 'period': [], 'ema': []}\n",
    "\n",
    "pt_coding = {}\n",
    "\n",
    "for i_pt, pt in enumerate(pt_sel):  # add index for pt coding\n",
    "    pt_coding[pt] = i_pt\n",
    "    for i_period, period in enumerate([PER1, PER2]):\n",
    "        values = data[period][pt]\n",
    "        values = values.loc[values['Submission'].astype(int) == 1]  # only take completed emas\n",
    "        values = values[Q_SEL].astype(float)  # take selected question\n",
    "        values = values[~np.isnan(values)]  # excl nan values\n",
    "        stat_df['ema'].extend(values)\n",
    "        stat_df['sub'].extend([i_pt] * len(values))\n",
    "        stat_df['period'].extend([i_period] * len(values))\n",
    "\n",
    "stat_df = pd.DataFrame(stat_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pt in np.unique(stat_df['sub']):\n",
    "\n",
    "    temp_df = stat_df[stat_df['sub'] == pt]\n",
    "    values = [temp_df[temp_df['period'] == p]['ema'] for p in [0, 1]]\n",
    "    plt.boxplot(values)\n",
    "\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### stats\n",
    "\n",
    "# define model\n",
    "lm_model = MixedLM(\n",
    "    endog=np.array(stat_df['period']),  # dependent variable \n",
    "    exog=np.array(stat_df['ema']),  # independent variable (i.e., LID presence, movement)\n",
    "    groups=np.array(stat_df['sub']),  # subjects\n",
    "    exog_re=None,  # (None)  defaults to a random intercept for each group\n",
    ")\n",
    "# run and fit model\n",
    "# try:\n",
    "lm_results = lm_model.fit()\n",
    "# except:\n",
    "#     if allow_lm_error:\n",
    "#         return False\n",
    "#     else:\n",
    "#         print(dep_var.shape, indep_var.shape, groups.shape)\n",
    "#         lm_results = lm_model.fit()\n",
    "\n",
    "# extract results\n",
    "fixeff_cf = lm_results._results.fe_params[0]\n",
    "pval = lm_results._results.pvalues[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_results._results.fe_params, lm_results._results.pvalues\n",
    "\n",
    "print(lm_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check means and variances for movement items, tremor, and gait items\n",
    "- split per sub\n",
    "- split per ses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df.EMA_reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(home_helpers)\n",
    "\n",
    "data = {}\n",
    "\n",
    "# Define pre-operative sessions\n",
    "sel_info = home_helpers.select_sessions()\n",
    "sel_info = sel_info.set_index(sel_info['study_id'],)\n",
    "sel_sessions = {sub: ses for sub, ses in sel_info[['study_id', 'Session']].values}\n",
    "print(sel_sessions)\n",
    "\n",
    "\n",
    "for sub, ses in sel_sessions.items():\n",
    "\n",
    "    data[sub] = loadSubject(\n",
    "        sub=sub,\n",
    "        ses=ses,\n",
    "        incl_EMA=True,\n",
    "        incl_ACC=False,\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Explore naturalistic ACC\n",
    "\n",
    "include loading option for ACC only for EMA windows, store these selected windows separately, to prevent loading of full acc data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feas_data_path = os.path.join(\n",
    "    os.path.dirname(load_utils.get_onedrive_path()),\n",
    "    'PROJECTS', 'home_feasibility'\n",
    ")\n",
    "feas_fig_path = os.path.join(\n",
    "    load_utils.get_onedrive_path('figures'),\n",
    "    'feasibility'\n",
    ")\n",
    "\n",
    "ntrl_fig_path = load_utils.get_onedrive_path('emaval_fig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load ACC data, create SVM and filtered data wihtin the dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import naturalistic data via dbs_home repo\n",
    "\n",
    "sub_id = 'hm24'\n",
    "ses_id = 'ses01'\n",
    "\n",
    "# dev_day_selection = ['2025-07-17', '2025-07-18']\n",
    "# dev_day_selection = [f'2025-07-{d}' for d in np.arange(17, 31)]\n",
    "dev_day_selection = []\n",
    "\n",
    "home_dat = loadSubject(\n",
    "    sub=sub_id,\n",
    "    ses=ses_id,\n",
    "    incl_STEPS=False,\n",
    "    incl_EPHYS=False,\n",
    "    incl_EMA=True,\n",
    "    incl_ACC=True,\n",
    "    day_selection=dev_day_selection\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check available EMAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_home_compl.plot_EMA_completion_perSession(home_dat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Acc-Windows aligned to EMAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dbs_home.preprocessing import acc_preprocessing as acc_prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "\n",
    "@dataclass(init=True,)\n",
    "class windowData:\n",
    "    sub: str\n",
    "    ses: str\n",
    "    day: str | None = None\n",
    "    acc_times: np.ndarray | None = None\n",
    "    acc_triax: np.ndarray | None = None\n",
    "    acc_svm: np.ndarray | None = None\n",
    "    sfreq: int | None = None\n",
    "    ema: dict = field(default_factory=dict)\n",
    "    day: str | None = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "\n",
    "        print(f'created windowData class for {self.sub}, {self.ses};'\n",
    "              f'starttime {self.acc_times[0]}')\n",
    "        if type(self.day) == str: print(f'belonging to day {self.day}')\n",
    "\n",
    "        if self.sfreq == None:\n",
    "            # extract sfreq if not given\n",
    "            time_df = np.diff(self.acc_times[:5])[0]\n",
    "            self.sfreq = int(dt.timedelta(seconds=1) / time_df)\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_ACC_PRESENT = 0.5\n",
    "WIN_SAMPLES = (15 * 60 * 32)\n",
    "# create dict with ACC data per EMA window\n",
    "\n",
    "win_dict_daily = {}  #  for plotting\n",
    "all_windows = []\n",
    "\n",
    "for i_day, str_day in enumerate(home_dat.watch_days):\n",
    "    # define current day\n",
    "    print(str_day)\n",
    "    \n",
    "    # if str_day != '2025-07-28': continue\n",
    "    \n",
    "    day_dict_lists = acc_prep.get_day_EMA_AccWindows(\n",
    "        subSesClass=home_dat, str_day=str_day,\n",
    "    )\n",
    "\n",
    "    # add key to store windows per day\n",
    "    win_dict_daily[str_day] = {'ema': [], 'acc_times': [], 'acc_svm': []}\n",
    "\n",
    "    for i_win in np.arange(len(list(day_dict_lists.values())[0])):\n",
    "        # create class with processed acc-data and with ema-dict per completed ema\n",
    "        \n",
    "        # skip incomplete acc data\n",
    "        if len(day_dict_lists['acc_times'][i_win]) < (WIN_SAMPLES * MIN_ACC_PRESENT): continue\n",
    "        \n",
    "        windat = windowData(sub=home_dat.sub,\n",
    "                            ses=home_dat.ses,\n",
    "                            day=str_day,\n",
    "                            acc_times=day_dict_lists['acc_times'][i_win],\n",
    "                            acc_triax=day_dict_lists['acc_filt'][i_win],\n",
    "                            acc_svm=day_dict_lists['acc_svm'][i_win],\n",
    "                            ema=day_dict_lists['ema'][i_win])\n",
    "        all_windows.append(windat)\n",
    "\n",
    "        # calculate features durectly here, to prevent sub storing\n",
    "        \n",
    "        # add data per window (for plotting currently)\n",
    "        for key in ['ema', 'acc_times', 'acc_svm']:\n",
    "            win_dict_daily[str_day][key].append(day_dict_lists[key][i_win])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check dict by plotting all windows for one day in same plot\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "\n",
    "ax_ema = ax.twinx()\n",
    "\n",
    "for ema, win_times, win_svm in zip(\n",
    "    win_dict_daily['2025-07-17']['ema'],\n",
    "    win_dict_daily['2025-07-17']['acc_times'],\n",
    "    win_dict_daily['2025-07-17']['acc_svm']\n",
    "):\n",
    "    ax.plot(win_times, win_svm)\n",
    "    \n",
    "    # scatter \"move well\"-value\n",
    "    ax_ema.scatter(win_times[0], ema['Q6'], color='gray', s=50, alpha=.5,)\n",
    "\n",
    "ax.set_ylabel('ACC SVM (vector-g)')\n",
    "ax_ema.set_ylabel('EMA answer (Likert-scale)')\n",
    "\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feature extraction\n",
    "\n",
    "- Hssayeni et al, Scientific Reports 2021\n",
    "    - strongest wrist-features: angular velocity, standard deviation, power of secondary frequency, power of 1â€“4 Hz band, and Shannon Entropy (r = 0.82  - r = 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import variation, pearsonr\n",
    "from scipy.signal import welch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import find_peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dbs_home.preprocessing import run_preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_id, ses_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: get submovement timings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(run_preprocessing)\n",
    "\n",
    "\n",
    "run_preprocessing.main(\n",
    "    sub_id=sub_id,\n",
    "    ses_id=ses_id,\n",
    "    lowcut=0.1,\n",
    "    highcut=15,\n",
    "    bout_threshold=50,\n",
    "    BOUT_METHOD='labels',\n",
    "    SAVE_SUBMOVE_IDXS = True,\n",
    "    save=True,\n",
    "    day_selection=['2025-07-20'],\n",
    "    preprocessing_version=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # def get_jerk_magn(seg_svm,):\n",
    "# #     \"\"\"\n",
    "# #     calculate jerk magnitude (derivative of svm) as \n",
    "# #     best approximation for angular velocity\n",
    "# #     \"\"\"\n",
    "\n",
    "# #     jerk_mag = np.abs(np.gradient(seg_svm))\n",
    "# #     jerk_mag = np.mean(jerk_mag)\n",
    "\n",
    "# #     return jerk_mag\n",
    "\n",
    "# # def get_svm_var(seg_svm):\n",
    "\n",
    "# #     coef_var = variation(seg_svm)\n",
    "\n",
    "# #     return coef_var\n",
    "\n",
    "# # def get_svm_sd(seg_svm):\n",
    "\n",
    "# #     stddev = np.std(seg_svm)\n",
    "\n",
    "# #     return stddev\n",
    "\n",
    "# # def get_psd(seg_svm, sfreq):\n",
    "\n",
    "# #     fx, psx = welch(x=seg_svm, fs=sfreq, nperseg=sfreq, noverlap=0,)\n",
    "\n",
    "# #     return fx, psx\n",
    "\n",
    "# # def get_pow_1_4(fx, psx):\n",
    "# #     \"\"\"\n",
    "# #     selects 1, 2, and 3 Hz\n",
    "# #     \"\"\"\n",
    "\n",
    "# #     f_sel = (fx >= 1) & (fx < 4)\n",
    "# #     pow = sum(psx[f_sel])\n",
    "\n",
    "# #     return pow\n",
    "\n",
    "# # def get_pow_4_8(fx, psx):\n",
    "\n",
    "# #     f_sel = (fx >= 4) & (fx <= 8)\n",
    "# #     pow = sum(psx[f_sel])\n",
    "\n",
    "# #     return pow\n",
    "\n",
    "# # def get_pow_2ndFreq(fx, psx, PEAK_WIDTH=1):\n",
    "    \n",
    "# #     f_peaks, peak_props = find_peaks(x=psx, height=np.min(psx),)\n",
    "# #     peak_idx_ordered = np.argsort(peak_props['peak_heights'])  # sorts from small to big\n",
    "# #     try:\n",
    "# #         f_2nd_peak = f_peaks[peak_idx_ordered[-2]]\n",
    "# #     except IndexError:\n",
    "# #         print(f_peaks, peak_props, peak_idx_ordered)\n",
    "# #         return 0\n",
    "    \n",
    "# #     # print(f'2nd peak found: {f_2nd_peak} Hz \\t(after first {f_peaks[peak_idx_ordered[-1]]} Hz)')\n",
    "# #     f_sel = (fx >= f_2nd_peak - PEAK_WIDTH) & (fx <= (f_2nd_peak + PEAK_WIDTH))\n",
    "# #     pow = sum(psx[f_sel])\n",
    "\n",
    "# #     return pow\n",
    "\n",
    "# # def get_dom_freq(fx, psx, PEAK_WIDTH=1):\n",
    "    \n",
    "# #     f_peaks, peak_props = find_peaks(x=psx, height=np.min(psx),)\n",
    "# #     i_max_peak = np.argmax(peak_props['peak_heights'])  # sorts from small to big\n",
    "# #     dom_freq = f_peaks[i_max_peak]\n",
    "\n",
    "# #     return dom_freq\n",
    "\n",
    "# # def get_corrcoef_components(triax_acc):\n",
    "\n",
    "# #     pca = PCA(n_components=3)\n",
    "# #     comps = pca.fit_transform(triax_acc)\n",
    "# #     corr_coef, _ = pearsonr(abs(comps[:, 0]), abs(comps[:, 1]))\n",
    "\n",
    "# #     return corr_coef\n",
    "\n",
    "\n",
    "# @dataclass\n",
    "# class featExtraction:\n",
    "#     \"\"\"\n",
    "#     extracts features from winData dataclass per segment,\n",
    "#     requires segment start index and segment length in samples\n",
    "#     \"\"\"\n",
    "#     seg_svm: np.ndarray\n",
    "#     seg_triax: np.ndarray | None = None\n",
    "#     sfreq: int = 32\n",
    "#     incl_feats: list = field(default_factory=[])\n",
    "#     feat_funcs = acc_fts\n",
    "\n",
    "\n",
    "#     def __post_init__(self):\n",
    "\n",
    "#         # self.get_jerk_magn = get_jerk_magn(self.seg_svm)\n",
    "\n",
    "#         # self.get_svm_var = get_svm_var(self.seg_svm)\n",
    "        \n",
    "#         # self.get_svm_sd = get_svm_sd(self.seg_svm)\n",
    "\n",
    "#         # self.fx, self.psx = get_psd(self.seg_svm, sfreq=self.sfreq,)\n",
    "        \n",
    "#         # self.get_pow_1_4 = get_pow_1_4(self.fx, self.psx)\n",
    "\n",
    "#         # self.get_pow_4_8 = get_pow_4_8(self.fx, self.psx)\n",
    "\n",
    "#         # self.get_pow_2ndFreq = get_pow_2ndFreq(self.fx, self.psx)\n",
    "\n",
    "#         # self.get_dom_freq = get_dom_freq(self.fx, self.psx)\n",
    "\n",
    "#         # self.get_corrcoef_components = get_corrcoef_components(self.seg_triax)\n",
    "\n",
    "#         # prepare psd in case spectral features inclued\n",
    "#         if any([f.startswith('pow') for f in self.incl_feats]):\n",
    "#             self.fx, self.psx = acc_fts.get_psd(self.seg_svm, sfreq=self.sfreq,)\n",
    "        \n",
    "#         seg_feats = []\n",
    "\n",
    "#         for ft in [ft for ft, sel in self.incl_feats.items() if sel]:\n",
    "#             print(ft)\n",
    "#             value = getattr(self.feat_funcs, f'get_{ft}')(self)\n",
    "#             seg_feats.append(value)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.acc_features as acc_fts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(acc_fts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_segment_features(\n",
    "    segData,\n",
    "    incl_feats: list = field(default_factory=[]),    \n",
    "):\n",
    "\n",
    "    # prepare psd in case spectral features inclued\n",
    "    if any(['pow' in f or 'freq' in f for f in incl_feats]):\n",
    "        (segData.fx, segData.psx) = acc_fts.get_psd(segData)\n",
    "    \n",
    "    seg_feats = []\n",
    "\n",
    "    for ft in [ft for ft, sel in incl_feats.items() if sel]:\n",
    "        value = getattr(acc_fts, f'get_{ft}')(segData)\n",
    "        seg_feats.append(value)\n",
    "    \n",
    "    return seg_feats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "def get_feats_for_window(\n",
    "    windatClass,\n",
    "    SEG_LEN_SEC: float = 1,\n",
    "    FEATS_INCL: dict = {\n",
    "        'svm_rms': True, 'jerk_magn': True,\n",
    "        'svm_sd': True, 'svm_var': True,\n",
    "        'ax_var': True, 'pow_1_4': True,\n",
    "        'pow_4_8': True, 'pow_2ndFreq': True,\n",
    "        'corrcoef_components': True,\n",
    "    },\n",
    "    featClass = None,\n",
    "    SUMMARY_METH = None,\n",
    "):\n",
    "    \n",
    "    if type(SUMMARY_METH) == str:\n",
    "        assert(SUMMARY_METH in ['mean', 'std', '90perc']), 'INCORRECT SUMM METHOD'\n",
    "\n",
    "    # extract segment length if not given\n",
    "    SEG_N_SAMPLES = SEG_LEN_SEC * windatClass.sfreq\n",
    "\n",
    "    win_feats = []  # for arr structure\n",
    "    sel_feats = [ft for ft, bl in FEATS_INCL.items() if bl == True]\n",
    "    # print(win_feats)\n",
    "\n",
    "    # calculate features per segment\n",
    "    segClass = dataclasses.replace(windatClass)\n",
    "\n",
    "    for i_start in np.arange(len(windatClass.acc_svm),\n",
    "                             step=SEG_N_SAMPLES):\n",
    "        # print(i_start)\n",
    "\n",
    "        # seg_fts = []\n",
    "        # featClass = featExtraction(\n",
    "        #     seg_svm=windatClass.acc_svm[i_start:i_start+SEG_N_SAMPLES],\n",
    "        #     seg_triax=windatClass.acc_triax[i_start:i_start+SEG_N_SAMPLES],\n",
    "        #     sfreq=windatClass.sfreq,\n",
    "        #     incl_feats=sel_feats,\n",
    "        # )\n",
    "        # for ft in sel_feats:\n",
    "        #     # array structure\n",
    "        #     ft_value = getattr(featClass, f\"get_{ft}\")\n",
    "        #     seg_fts.append(ft_value)\n",
    "        \n",
    "        i_end = i_start + SEG_N_SAMPLES\n",
    "        for att in ['acc_times', 'acc_svm', 'acc_triax']:\n",
    "            setattr(segClass, att, getattr(windatClass, att)[i_start:i_end])\n",
    "\n",
    "        seg_fts = get_segment_features(segData=segClass,\n",
    "                                       incl_feats=FEATS_INCL)\n",
    "        # add all feats per segment\n",
    "        win_feats.append(seg_fts)\n",
    "    \n",
    "    win_feats = np.array(win_feats)  # return values as one array\n",
    "        \n",
    "    if type(SUMMARY_METH) == str:\n",
    "        if SUMMARY_METH == 'mean':\n",
    "            win_feats = np.mean(win_feats, axis=0,)\n",
    "        elif SUMMARY_METH == 'std':\n",
    "            win_feats = np.std(win_feats, axis=0,)\n",
    "        elif SUMMARY_METH == '90perc':\n",
    "            win_feats = np.percentile(win_feats, 90, axis=0,)\n",
    "\n",
    "\n",
    "    return win_feats, sel_feats\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "vars(windat).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "FEATS_INCL = {\n",
    "    'jerk_magn': True,\n",
    "    'svm_sd': True,\n",
    "    'svm_var': True,\n",
    "    'pow_1_4': True,\n",
    "    'pow_4_8': True, \n",
    "    'pow_2ndFreq': True,\n",
    "    'corrcoef_components': True,\n",
    "}\n",
    "EMA_TARGET = 'Q8'\n",
    "\n",
    "### create X-array with features per window\n",
    "X_lists, y_lists = [], []\n",
    "day_list = []  # for grouping random variables\n",
    "\n",
    "for i_win, windat in enumerate(all_windows):\n",
    "    \n",
    "    print(f'start # {i_win+1} / {len(all_windows)}')\n",
    "\n",
    "    tempfts, ft_keys = get_feats_for_window(\n",
    "        windatClass=windat,\n",
    "        FEATS_INCL=FEATS_INCL,\n",
    "        SUMMARY_METH='90perc',  # 90perc, mean\n",
    "        SEG_LEN_SEC=3,\n",
    "    )\n",
    "\n",
    "    X_lists.append(tempfts)\n",
    "\n",
    "\n",
    "    ### create y-array with EMA-label per window\n",
    "    y_lists.append(float(windat.ema[EMA_TARGET]))\n",
    "\n",
    "    day_list.append(windat.day)\n",
    "\n",
    "# make arrays\n",
    "X_arr = np.array(X_lists)\n",
    "# zscore\n",
    "m, sd = np.mean(X_arr, axis=0), np.std(X_arr, axis=0)\n",
    "X_arr[:, :] -= m\n",
    "X_arr[:, :] /= sd\n",
    "\n",
    "y_arr = np.array(y_lists)\n",
    "\n",
    "uniq_days = np.unique(day_list)\n",
    "day_codes = {str(d): i_day for i_day, d in enumerate(uniq_days)}\n",
    "daycode_arr = np.array([day_codes[d] for d in day_list])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "### test pca idea\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "comps = pca.fit_transform(windat.acc_triax)\n",
    "stat, p = pearsonr(abs(comps[:, 0]), abs(comps[:, 1]))\n",
    "\n",
    "print('explained variance ratio', pca.explained_variance_ratio_)\n",
    "print('singular values', pca.singular_values_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(abs(comps[:, 0]))\n",
    "plt.plot(abs(comps[:, 1]))\n",
    "plt.show()\n",
    "\n",
    "stat, p = pearsonr(abs(comps[:, 0]), abs(comps[:, 1]))\n",
    "\n",
    "stat, p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.regression.mixed_linear_model import MixedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# statdf = np.concat([X_arr, y_arr.reshape(-1, 1)], axis=1)\n",
    "# stat_df = pd.DataFrame(statdf, columns=ft_keys + ['EMA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random intercepts only\n",
    "model = smf.mixedlm(\n",
    "    f\"UPDRS_SUM_{motor_target} ~ {lmm_fix[FIX_EFF]}\",\n",
    "    lmm_df,\n",
    "    # groups=lmm_df[\"subid\"],\n",
    "    # re_formula=f\"~EMA_SUM_{motor_target}\",  # for random slopes for EMA motor\n",
    ")\n",
    "result = model.fit()\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_f, f in enumerate(ft_keys):\n",
    "    \n",
    "    res = spearmanr(X_arr[:, i_f], y_arr)\n",
    "\n",
    "    print(f'\\n{f}: {res.statistic.round(2)}, p = {res.pvalue.round(5)}')\n",
    "\n",
    "    plt.scatter(y_arr, X_arr[:, i_f])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MixedLM(endog=y_arr, exog=X_arr, groups=daycode_arr)\n",
    "result = model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.fe_params\n",
    "\n",
    "result.cov_re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, r2_score,\n",
    "    roc_auc_score, balanced_accuracy_score\n",
    ")\n",
    "from scipy.stats import f as stats_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_f_stat(y_pred, y_true, n_feats):\n",
    "    # F-statistic for model usability\n",
    "    \n",
    "    # Sum of squares\n",
    "    SSR = np.sum((y_pred - np.mean(y_true)) ** 2)   # Regression\n",
    "    SSE = np.sum((y_true - y_pred) ** 2)   # Error\n",
    "    SST = np.sum((y_true - np.mean(y_true)) ** 2)   # Total\n",
    "\n",
    "    # Degrees of freedom\n",
    "    df_reg = n_feats\n",
    "    df_err = len(y_true) - (n_feats + 1)  # +1 for coeff next to betas of features\n",
    "\n",
    "    # Mean squares\n",
    "    MSR = SSR / df_reg if df_reg > 0 else np.nan\n",
    "    MSE = SSE / df_err if df_err > 0 else np.nan\n",
    "    F = MSR / MSE\n",
    "    # p-value for the observed F\n",
    "    f_pval = 1 - stats_f.cdf(F, df_reg, df_err)\n",
    "\n",
    "    return F, f_pval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLSF = 'lda'\n",
    "\n",
    "TEST_SEL = daycode_arr > 20  # takes circa .33\n",
    "\n",
    "X_train = X_arr[~TEST_SEL, :]\n",
    "y_train = y_arr[~TEST_SEL]\n",
    "\n",
    "# test cohort\n",
    "X_test = X_arr[TEST_SEL, :]\n",
    "y_true = y_arr[TEST_SEL]\n",
    "\n",
    "# MAKE BINARY\n",
    "if CLSF == 'bin':\n",
    "    y_train = y_train >= 5\n",
    "    y_true = y_true >= 5\n",
    "\n",
    "# Run prediction\n",
    "models = {'scale': LinearRegression(),\n",
    "          'lda': LDA(),\n",
    "          'bin': LogisticRegression()}\n",
    "model = models[CLSF]\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "# round predictions to full numbers\n",
    "if CLSF == 'scale' or 'lda': \n",
    "    y_pred = np.array([np.round(v) for v in y_pred])\n",
    "\n",
    "    pred_F, pred_F_p = get_f_stat(y_pred=y_pred, y_true=y_true, n_feats=X_test.shape[1])\n",
    "    pred_corrcoef, prs_p = pearsonr(y_true, y_pred)\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    R2 = r2_score(y_true=y_true, y_pred=y_pred,)\n",
    "\n",
    "    print(f'({CLSF}) accuracy: {np.round(acc, 2)} (test sample: n={len(y_true)})')\n",
    "    print(f'({CLSF}) R2: {np.round(R2, 2)} (test sample: n={len(y_true)})')\n",
    "    print(f'({CLSF}) F-stat: {np.round(pred_F, 2)}, p={pred_F_p}')\n",
    "\n",
    "\n",
    "elif  CLSF == 'bin':\n",
    "    auc = roc_auc_score(y_true=y_true, y_score=y_pred)\n",
    "    print(f'({CLSF}) AUROC: {np.round(auc, 2)} (test sample: n={len(y_true)})')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_true)\n",
    "plt.plot(y_pred)\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(y_true + np.random.uniform(-.2, .2, len(y_true)),\n",
    "            y_pred + np.random.uniform(-.2, .2, len(y_pred)),)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Corr Coeff: {prs_stat.round(2)}, p = {prs_p.round(4)}')\n",
    "\n",
    "plt.scatter(y_true, y_pred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perm test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TEST_SEL = daycode_arr > 20  # takes circa .33\n",
    "\n",
    "X_train = X_arr[~TEST_SEL, :]\n",
    "y_train = y_arr[~TEST_SEL]\n",
    "\n",
    "# test cohort\n",
    "X_test = X_arr[TEST_SEL, :]\n",
    "y_true = y_arr[TEST_SEL]\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "# Run prediction\n",
    "perm_mets ={'F': [], 'R': []}\n",
    "\n",
    "np.random.seed(27)\n",
    "\n",
    "for i_perm in np.arange(10000):\n",
    "    y_train = y_arr[~TEST_SEL]\n",
    "    np.random.shuffle(y_train)   \n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_perm_pred = model.predict(X_test)\n",
    "    # np.random.shuffle(y_perm_pred)\n",
    "    y_perm_pred = np.array([np.round(v) for v in y_perm_pred])\n",
    "\n",
    "    F, f_pvalue = get_f_stat(y_pred=y_perm_pred, y_true=y_true,\n",
    "                             n_feats=X_test.shape[1])\n",
    "    prs_stat, _ = pearsonr(y_perm_pred, y_true)\n",
    "    perm_mets['F'].append(F)\n",
    "    perm_mets['R'].append(prs_stat)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2)\n",
    "\n",
    "pred_mets = {'F': pred_F, 'R': pred_corrcoef}\n",
    "\n",
    "for i_ax, metr in enumerate(list(perm_mets.keys())):\n",
    "\n",
    "    axes[i_ax].hist(perm_mets[metr], color='gray', alpha=.5,)\n",
    "    axes[i_ax].axvline(np.percentile(perm_mets[metr], 97.5),\n",
    "                       color='purple', alpha=.5, lw=1, )\n",
    "    \n",
    "    axes[i_ax].axvline(pred_mets[metr], color='orange',\n",
    "                       alpha=.8, lw=3, )\n",
    "    \n",
    "    p_calc = sum(pred_mets[metr] < perm_mets[metr]) / len(perm_mets[metr])\n",
    "    print(f'metric {metr}: p = {np.round(p_calc, 3)}')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get submovement indices\n",
    "\n",
    "use per window:\n",
    "- n submovements / min\n",
    "- mean length\n",
    "- variation of lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over each day\n",
    "for day_idx, day in enumerate(home_dat.watch_days):\n",
    "    \n",
    "    # if day_idx > 0: continue  # ONLY EXECUTE ONE DAY FOR DEVELOPMENT\n",
    "    \n",
    "    print(f'\\nStart day i-{day_idx}: {day}')\n",
    "    print('load acc day')\n",
    "    day_accClass = moveProc.get_accel_day(dat=home_dat, day_index=day_idx,)\n",
    "\n",
    "   \n",
    "\n",
    "    print('compute act index')\n",
    "    # calculate act index\n",
    "    ai_values, ai_times = moveProc.compute_activity_index(\n",
    "        day_accClass.raw_triax_acc,\n",
    "        day_accClass.timestamps\n",
    "    )\n",
    "    print('cluster ai')\n",
    "    # cluster act indices\n",
    "    kmeans = KMeans(\n",
    "        n_clusters=2, random_state=0, n_init=\"auto\"\n",
    "    ).fit(ai_values.reshape([-1, 1]))\n",
    "\n",
    "    ai_binary_clusters = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(load_data)\n",
    "importlib.reload(prep_data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# load all combined data\n",
    "\n",
    "# SUBS_INCL = ['hm14']\n",
    "\n",
    "\n",
    "emadata = {m: {} for m in MOMENTS}\n",
    "accdata = {m: {} for m in MOMENTS}\n",
    "\n",
    "for rec_moment, sub_sess in sessions_include.items():\n",
    "\n",
    "    for sub, ses in sub_sess.items():\n",
    "        print(f'\\n\\n{\"#\" * 30}\\nstart sub-{sub}: ses:{ses}\\n{\"#\" * 30}\\n\\n')\n",
    "\n",
    "        # to test acc loading\n",
    "        # if not sub in SUBS_INCL: continue\n",
    "\n",
    "        if sub in sub_skip: continue\n",
    "                    \n",
    "        if [sub, ses] in ses_skip: continue\n",
    "                \n",
    "        ses_class = load_home.loadSubject(\n",
    "            sub=sub,\n",
    "            ses=ses,\n",
    "            incl_EMA=True,\n",
    "            incl_ACC=True,\n",
    "        )\n",
    "        # temp_df = ema_utils.load_ema_df(sub_ses_class=ses_class)\n",
    "        # # prepare\n",
    "        # temp_df = prep_data.prepare_ema_df(temp_df, ADD_MEANMOVE=True, INVERT_NEG_ITEMS=False,)\n",
    "        ### TODO: CHECK WHY NOT ALL SUBS ARE INVERTED PROPERLY\n",
    "\n",
    "        # emadata[rec_moment][sub] = temp_df\n",
    "\n",
    "        ################\n",
    "        # store and plot acc feasib acquisition\n",
    "        \n",
    "        # get feas/acquisition numbers\n",
    "        sub_timesums = get_acc_feas(ses_class)\n",
    "\n",
    "        # Save with highest protocol (fast & compact)\n",
    "        fname = f\"acc_seconds_{ses_class.sub}_{ses_class.ses}.pkl\"\n",
    "\n",
    "        with open(os.path.join(feas_data_path, fname), \"wb\") as f:\n",
    "            pickle.dump(sub_timesums, f,\n",
    "                        protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        plot_feas_acc(sub_timesums=sub_timesums,\n",
    "                      sub=ses_class.sub, ses=ses_class.ses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feas_acc(sub_timesums=feasload, sub=SUB, ses=SES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(load_watch)\n",
    "importlib.reload(load_home)\n",
    "importlib.reload(dbs_home)\n",
    "\n",
    "\n",
    "temp = load_home.loadSubject(\n",
    "            sub='hm23',\n",
    "            ses='ses01',\n",
    "            incl_EMA=True,\n",
    "            incl_ACC=True,\n",
    "            proc_ACC=False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dbs_home.utils.finding_paths as home_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.sub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### explore kinematic features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create dict with ACC data per EMA window\n",
    "\n",
    "acc_ema_combis = {}\n",
    "\n",
    "for i_day, str_day in enumerate(home_dat.watch_days):\n",
    "    # define current day\n",
    "    print(str_day)\n",
    "    # check default parameters here\n",
    "    acc_ema_combis[str_day] = acc_prep.get_day_EMA_AccWindows(\n",
    "        subSesClass=home_dat, str_day=str_day,\n",
    "    )\n",
    "    \n",
    "\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMA_PICK = {'LID': 'Q8'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ema_values = {k: [] for k in EMA_PICK.keys()}\n",
    "feats = {}\n",
    "\n",
    "# day_i = 0\n",
    "# day_key = list(acc_ema_combis.keys())[day_i]\n",
    "\n",
    "for day_i, (day_key, day_dict) in enumerate(\n",
    "    acc_ema_combis.items()\n",
    "):\n",
    "\n",
    "# ema_i = 0\n",
    "    for ema_i in np.arange(len(acc_ema_combis[day_key]['ema'])):\n",
    "\n",
    "        # ADD SELECTED EMA values\n",
    "        for key, code in EMA_PICK.items():\n",
    "            ema_values[key].append(\n",
    "                acc_ema_combis[day_key]['ema'][ema_i][code]\n",
    "            )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ema_values['LID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "explore feasbility, completion rates for acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc_feas(\n",
    "    dataClass_acc,\n",
    "    TIME_GAP_SEC = 1,\n",
    "    ACC_SFREQ = 32,\n",
    "):\n",
    "\n",
    "    sub_timesums = {}\n",
    "\n",
    "    for i_day, day in enumerate(dataClass_acc.watch_days):\n",
    "        print(f'\\n\\n{day}')\n",
    "        time_sum = dt.timedelta(0)  # store collected time sum in variable dt timedelta\n",
    "\n",
    "        time_diff = np.diff(dataClass_acc.acc_times[i_day][::(ACC_SFREQ * TIME_GAP_SEC)])  # use every 1-second value\n",
    "\n",
    "        per_start, per_end = None, None\n",
    "\n",
    "        for t_df, stamp in zip(\n",
    "            time_diff,\n",
    "            dataClass_acc.acc_times[i_day][::(ACC_SFREQ * TIME_GAP_SEC)]\n",
    "        ):  # loop over timedeltas\n",
    "\n",
    "            if not per_start:\n",
    "                per_start = stamp\n",
    "                # print(f'\\nstartis {per_start}')\n",
    "                continue\n",
    "\n",
    "            # if too large time gap\n",
    "            if t_df > dt.timedelta(seconds=TIME_GAP_SEC):\n",
    "                per_end = stamp\n",
    "                # add period times to list\n",
    "                time_sum += (per_end - per_start)\n",
    "                # reset period times and start over\n",
    "                per_start, per_end = None, None\n",
    "\n",
    "        if per_start and not per_end:\n",
    "            per_end = stamp\n",
    "            # add period times to list\n",
    "            time_sum += (per_end - per_start)\n",
    "            \n",
    "        # store day sum in sub dict\n",
    "        sub_timesums[day] = time_sum\n",
    "\n",
    "    return sub_timesums\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load\n",
    "\n",
    "SUB='hm23'\n",
    "SES='ses01'\n",
    "\n",
    "fname = f\"acc_seconds_{SUB}_{SES}.pkl\"\n",
    "\n",
    "with open(os.path.join(feas_data_path, fname), \"rb\") as f:\n",
    "    feasload = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feasload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feas_acc(sub_timesums, sub, ses):\n",
    "\n",
    "       fname = f'ACC_collection_{sub}_{ses}'\n",
    "\n",
    "       fig, ax = plt.subplots(1, 1, figsize=(9, 3))\n",
    "\n",
    "       ax.bar(x=np.arange(len(sub_timesums)),\n",
    "              height=[t.seconds / 3600 for t in sub_timesums.values()])\n",
    "\n",
    "       ax.set_xticklabels([l for l in sub_timesums.keys()],\n",
    "                     rotation=45,)\n",
    "       ax.set_xticks(np.arange(len(sub_timesums.values())),)\n",
    "       ax.set_ylabel('Day sum (hours)')\n",
    "       ax.set_title(f'collected ACC-time: {sub}, {ses}')\n",
    "\n",
    "       plt.tight_layout()\n",
    "\n",
    "       plt.savefig(os.path.join(feas_fig_path, fname), dpi=300,\n",
    "                     facecolor='w', )\n",
    "\n",
    "       plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocess data: explore and visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get (mean-corrected) EMA and UPDRS values per symptom subtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(load_data)\n",
    "importlib.reload(prep_data)\n",
    "\n",
    "predat = ema_dat['hm14'].copy()\n",
    "# dat = prep_data.prepare_home_emas(predat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ema_dat['hm20'].columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Explore / visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figpath = load_utils.get_onedrive_path('emaval_fig')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ema_dat['hm18'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sub, dat in ema_dat.items():\n",
    "\n",
    "    daystart = dat['dates'].iloc[0]\n",
    "\n",
    "    dayend = dat['dates'].iloc[-1]\n",
    "\n",
    "    daystart = sel_info.loc[sub]['onboarding_date'] + dt.timedelta(days=1)\n",
    "    dayend = sel_info.loc[sub]['checkout_date'] - dt.timedelta(days=1)\n",
    "\n",
    "\n",
    "    ndays = (dayend - daystart).days\n",
    "\n",
    "    \n",
    "    compl_perc = dat.shape[0] / (ndays * 6)\n",
    "\n",
    "    print(f'{sub}: over {ndays} days completed {np.round(compl_perc, 2)}')\n",
    "\n",
    "\n",
    "# ema_dat['hm18']['dates']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Perform Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.stats as utilsstat\n",
    "import statsmodels.formula.api as smf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "home",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
