{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naturalistic EMA validation\n",
    "\n",
    "Applying the findings from the 4-state correlation work (EMA x UPDRS) onto real-life EMA data.\n",
    "\n",
    "Goals:\n",
    "- analyse real-life variation of EMA values\n",
    "    - inter-individual variation\n",
    "    - intra-individual variation, daily fluctuations, differences between days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Import packages\n",
    "\n",
    "- document versions for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import json\n",
    "import importlib\n",
    "from itertools import product, compress\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr, variation\n",
    "\n",
    "from dataclasses import dataclass, field\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Python sys', sys.version)\n",
    "print('pandas', pd.__version__)\n",
    "print('numpy', np.__version__)\n",
    "# print('mne_bids', mne_bids.__version__)\n",
    "# print('mne', mne.__version__)\n",
    "# print('sci-py', scipy.__version__)\n",
    "# print('sci-kit learn', sk.__version__)\n",
    "# print('matplotlib', plt_version)\n",
    "\n",
    "\"\"\"\n",
    "Python sys 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]\n",
    "pandas 2.1.1\n",
    "numpy 1.26.0\n",
    "\n",
    "from 16.09\n",
    "\n",
    "Python sys 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]\n",
    "pandas 2.3.2\n",
    "numpy 2.3.3\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import custom functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dbs_home repo\n",
    "from dbs_home.load_raw.main_load_raw import loadSubject \n",
    "import dbs_home.utils.helpers as home_helpers\n",
    "import dbs_home.utils.ema_utils as home_ema_utils\n",
    "import dbs_home.plot_data.plot_compliance as plot_home_compl\n",
    "import dbs_home.preprocessing.preparing_ema as home_ema_prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from current repo\n",
    "from utils import load_utils, load_data, prep_data\n",
    "from plotting import plot_help\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Home-Data\n",
    "\n",
    "Use pre-operative sessions\n",
    "\n",
    "- use 9-point-converter\n",
    "- use direct. inverter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import EMA home data from raw files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MOMENTS = ['pre-op', 'pre 3MFU', 'post 3MFU']\n",
    "\n",
    "sub_skip = [] # ['hm25',]  # skip full subject\n",
    "# skip per session\n",
    "ses_skip = [['hm20', 'ses03'],]\n",
    "# ses_skip = [['hm14', 'ses03']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions_include = {m: {} for m in MOMENTS}\n",
    "\n",
    "for rec_moment in MOMENTS:\n",
    "\n",
    "    sel_info = home_helpers.select_sessions(target_session=rec_moment)\n",
    "    sel_info = sel_info.set_index(sel_info['study_id'],)\n",
    "    sel_sessions = {sub: ses for sub, ses in sel_info[['study_id', 'Session']].values}\n",
    "\n",
    "    for key, val in sel_sessions.items():\n",
    "        sessions_include[rec_moment][key] = val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sessions_include.keys())\n",
    "print(sessions_include)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load EMA - data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(home_ema_utils)\n",
    "importlib.reload(load_data)\n",
    "importlib.reload(prep_data)\n",
    "importlib.reload(home_ema_prep)\n",
    "\n",
    "# load all combined data\n",
    "\n",
    "# SUBS_INCL = ['hm14']\n",
    "\n",
    "\n",
    "data = {m: {} for m in MOMENTS}\n",
    "\n",
    "for rec_moment, sub_sess in sessions_include.items():\n",
    "    # rec_moment contains 'pre-op', or 'pre 3MFU', 'post 3MFU', etc\n",
    "    for sub, ses in sub_sess.items():\n",
    "\n",
    "        if sub in sub_skip: continue\n",
    "                    \n",
    "        if [sub, ses] in ses_skip: continue\n",
    "                \n",
    "        ses_class = loadSubject(\n",
    "            sub=sub,\n",
    "            ses=ses,\n",
    "            incl_EMA=True,\n",
    "            incl_ACC=False,\n",
    "        )\n",
    "        temp_df = home_ema_utils.load_ema_df(sub_ses_class=ses_class)\n",
    "        # prepare\n",
    "        temp_df = home_ema_prep.prepare_ema_df(temp_df, ADD_MEANMOVE=True, INVERT_NEG_ITEMS=False,)\n",
    "        ### TODO: CHECK WHY NOT ALL SUBS ARE INVERTED PROPERLY\n",
    "\n",
    "        data[rec_moment][sub] = temp_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Explore naturalistic EMAs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess EMA\n",
    "\n",
    "- merge scores\n",
    "- invert negative-items (higher = clinically better)\n",
    "- mean-correct EMA\n",
    "    - test different normalizations:\n",
    "        - normalize with grand-mean per sub\n",
    "        - normalize with session mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(prep_data)\n",
    "\n",
    "MOMENTS = ['pre-op', 'pre 3MFU', 'post 3MFU']\n",
    "\n",
    "allsubs = []\n",
    "for mom in list(data.keys()): allsubs.extend(list(data[mom].keys()))\n",
    "allsubs = np.unique(allsubs)\n",
    "\n",
    "\n",
    "corr_data = {m: {} for m in MOMENTS}\n",
    "\n",
    "for sub in allsubs:\n",
    "\n",
    "    subdf = home_ema_prep.merge_sub_ema_df(datadict=data, sub=sub)\n",
    "    subdf = home_ema_prep.mean_correct_ema_df(subdf)\n",
    "\n",
    "    # split and palce back as moment dfs    \n",
    "    for moment in MOMENTS:\n",
    "        corr_data[moment][sub] = subdf[subdf['moment'] == moment].reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize first EMA results full group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLOT_ITEMS = ['move_mean', 'walking', 'tremor']\n",
    "\n",
    "PLOT_CORR = False\n",
    "if PLOT_CORR: PLOT_DATADICT = corr_data\n",
    "else: PLOT_DATADICT = data\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(9, 6))\n",
    "fname = 'motorItems_abs_perSub_perSes_1104'\n",
    "if PLOT_CORR: fname = fname.replace('abs', 'corr')\n",
    "\n",
    "x_margin = 2\n",
    "bin_w = 0.5\n",
    "\n",
    "fsize=14\n",
    "\n",
    "x_starts = {list(data.keys())[0]: 0}  # first moment starts at 0\n",
    "\n",
    "subcolors = plot_help.get_sub_colors(PLOT_DATADICT)\n",
    "\n",
    "\n",
    "for i_ax, col in enumerate(PLOT_ITEMS):\n",
    "\n",
    "    for i_mom, moment in enumerate(PLOT_DATADICT.keys()):\n",
    "\n",
    "        if PLOT_CORR and i_mom == 0: col = f'{col}_corr'\n",
    "\n",
    "        # loop over sub-dfs within moment and add specific column values\n",
    "        list_values = [tempdf[col].values for tempdf in PLOT_DATADICT[moment].values()]\n",
    "        box_subs = list(PLOT_DATADICT[moment].keys())  # subs included in this boxplot\n",
    "        # sort by sub id\n",
    "        i_sort = np.argsort(box_subs)\n",
    "        box_subs = [box_subs[i] for i in i_sort]\n",
    "        list_values = [list_values[i] for i in i_sort]\n",
    "\n",
    "        # drop NaN values in lists\n",
    "        list_values = [[v for v in l if not np.isnan(v)] for l in list_values]\n",
    "\n",
    "        # plot boxes for one moment\n",
    "        bp = axes[i_ax].boxplot(list_values, widths=bin_w,\n",
    "                           positions=x_starts[moment] + bin_w * np.arange(len(list_values)),\n",
    "                           patch_artist=True,)\n",
    "        if i_ax == 0:\n",
    "            if moment != list(PLOT_DATADICT.keys())[-1]:\n",
    "                x_starts[list(PLOT_DATADICT.keys())[i_mom + 1]] = x_starts[moment] + len(list_values) * bin_w + x_margin\n",
    "\n",
    "        # Loop over boxes\n",
    "        for patch, patchsub in zip(bp['boxes'], box_subs):\n",
    "            patch.set_facecolor(subcolors[patchsub])\n",
    "\n",
    "    # pretty plot\n",
    "    axes[i_ax].set_ylabel(f'{col}\\n(EMA answer)', size=fsize,)\n",
    "\n",
    "# pretty axes\n",
    "for ax in axes:\n",
    "    if not PLOT_CORR:\n",
    "        ax.set_ylim(0, 10)\n",
    "        ax.set_yticks(np.arange(1, 10, 2))\n",
    "        ax.set_yticklabels(np.arange(1, 10, 2))\n",
    "    else:\n",
    "        ax.set_ylim(-5, 5)\n",
    "        ax.set_yticks(np.arange(-4, 6, 2))\n",
    "        ax.set_yticklabels(np.arange(-4, 6, 2))\n",
    "\n",
    "    ax.set_xticks(list(x_starts.values()))\n",
    "    ax.set_xticklabels(list(x_starts.keys()))\n",
    "    ax.spines[['right', 'top']].set_visible(False)\n",
    "    ax.tick_params(axis='both', labelsize=fsize, size=fsize,)\n",
    "\n",
    "    ax.axhline(0, xmin=0, xmax=1,\n",
    "               color='gray', alpha=.3, zorder=0,)\n",
    "    if PLOT_CORR: ylines = [-4, -2, 2, 4,]\n",
    "    else: ylines = [1, 3, 5, 7, 9]\n",
    "    for yline in ylines:\n",
    "        ax.axhline(yline, xmin=0, xmax=1, color='gray', alpha=.15, zorder=0,)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# plt.savefig(os.path.join(load_utils.get_onedrive_path('figures'),\n",
    "#              'ema_naturalistic', fname),\n",
    "#              dpi=300, facecolor='w',)\n",
    "\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check completion rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for rec_moment in data.keys():\n",
    "\n",
    "    for sub in data[rec_moment].keys():\n",
    "\n",
    "        df = data[rec_moment][sub]\n",
    "\n",
    "        df['Submission'] = pd.to_numeric(df['Submission'], errors='coerce')\n",
    "        rate = df['Submission'].mean()\n",
    "        print(f\"{sub} completion rate @ {rec_moment}: {rate:.0%}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sub-analysis paper: EMA pre-post vs UPDRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.regression.mixed_linear_model import MixedLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_SEL = 'general movement'\n",
    "\n",
    "PER1 = 'pre-op'\n",
    "PER2 = 'post 3MFU'\n",
    "\n",
    "pt_pre = list(data[PER1].keys())\n",
    "pt_sel = [p for p in list(data[PER2].keys()) if p in pt_pre]\n",
    "\n",
    "stat_df = {'sub': [], 'period': [], 'ema': []}\n",
    "\n",
    "pt_coding = {}\n",
    "\n",
    "for i_pt, pt in enumerate(pt_sel):  # add index for pt coding\n",
    "    pt_coding[pt] = i_pt\n",
    "    for i_period, period in enumerate([PER1, PER2]):\n",
    "        values = data[period][pt]\n",
    "        values = values.loc[values['Submission'].astype(int) == 1]  # only take completed emas\n",
    "        values = values[Q_SEL].astype(float)  # take selected question\n",
    "        values = values[~np.isnan(values)]  # excl nan values\n",
    "        stat_df['ema'].extend(values)\n",
    "        stat_df['sub'].extend([i_pt] * len(values))\n",
    "        stat_df['period'].extend([i_period] * len(values))\n",
    "\n",
    "stat_df = pd.DataFrame(stat_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pt in np.unique(stat_df['sub']):\n",
    "\n",
    "    temp_df = stat_df[stat_df['sub'] == pt]\n",
    "    values = [temp_df[temp_df['period'] == p]['ema'] for p in [0, 1]]\n",
    "    plt.boxplot(values)\n",
    "\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### stats\n",
    "\n",
    "# define model\n",
    "lm_model = MixedLM(\n",
    "    endog=np.array(stat_df['period']),  # dependent variable \n",
    "    exog=np.array(stat_df['ema']),  # independent variable (i.e., LID presence, movement)\n",
    "    groups=np.array(stat_df['sub']),  # subjects\n",
    "    exog_re=None,  # (None)  defaults to a random intercept for each group\n",
    ")\n",
    "# run and fit model\n",
    "# try:\n",
    "lm_results = lm_model.fit()\n",
    "# except:\n",
    "#     if allow_lm_error:\n",
    "#         return False\n",
    "#     else:\n",
    "#         print(dep_var.shape, indep_var.shape, groups.shape)\n",
    "#         lm_results = lm_model.fit()\n",
    "\n",
    "# extract results\n",
    "fixeff_cf = lm_results._results.fe_params[0]\n",
    "pval = lm_results._results.pvalues[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_results._results.fe_params, lm_results._results.pvalues\n",
    "\n",
    "print(lm_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check means and variances for movement items, tremor, and gait items\n",
    "- split per sub\n",
    "- split per ses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df.EMA_reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(home_helpers)\n",
    "\n",
    "data = {}\n",
    "\n",
    "# Define pre-operative sessions\n",
    "sel_info = home_helpers.select_sessions()\n",
    "sel_info = sel_info.set_index(sel_info['study_id'],)\n",
    "sel_sessions = {sub: ses for sub, ses in sel_info[['study_id', 'Session']].values}\n",
    "print(sel_sessions)\n",
    "\n",
    "\n",
    "for sub, ses in sel_sessions.items():\n",
    "\n",
    "    data[sub] = loadSubject(\n",
    "        sub=sub,\n",
    "        ses=ses,\n",
    "        incl_EMA=True,\n",
    "        incl_ACC=False,\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Explore naturalistic ACC and Extract Features\n",
    "\n",
    "include loading option for ACC only for EMA windows, store these selected windows separately, to prevent loading of full acc data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dbs_home.preprocessing import acc_preprocessing as acc_prep\n",
    "from dbs_home.preprocessing import get_submovements\n",
    "import dbs_home.preprocessing.submovement_processing as submove_proc\n",
    "import dbs_home.load_raw.load_watch_raw as load_watch\n",
    "\n",
    "# current repo imports\n",
    "import utils.acc_features as acc_fts\n",
    "import utils.feat_extraction as ft_extr\n",
    "import utils.data_handling_ema_acc as data_handling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feas_data_path = os.path.join(\n",
    "    os.path.dirname(load_utils.get_onedrive_path()),\n",
    "    'PROJECTS', 'home_feasibility'\n",
    ")\n",
    "feas_fig_path = os.path.join(\n",
    "    load_utils.get_onedrive_path('figures'),\n",
    "    'feasibility'\n",
    ")\n",
    "\n",
    "ntrl_fig_path = load_utils.get_onedrive_path('emaval_fig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load ACC data\n",
    "- create SVM\n",
    "- filter data within the dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import naturalistic data via dbs_home repo\n",
    "\n",
    "# LID\n",
    "sub_id = 'hm24'\n",
    "ses_id = 'ses01'\n",
    "\n",
    "# # tremor check for Anna\n",
    "# sub_id = 'hm22'\n",
    "# ses_id = 'ses01'\n",
    "\n",
    "### test days for hm24-ses01  # dyskinesia\n",
    "# dev_day_selection = ['2025-07-17', '2025-07-18']\n",
    "# dev_day_selection = [f'2025-07-{d}' for d in np.arange(17, 31)]\n",
    "dev_day_selection = []\n",
    "\n",
    "### test days for hm20-ses01  # tremor\n",
    "# dev_day_selection = [\n",
    "#     '2025-06-13', '2025-06-14',\n",
    "#     '2025-06-15', '2025-06-16'\n",
    "# ]\n",
    "\n",
    "home_dat = loadSubject(\n",
    "    sub=sub_id,\n",
    "    ses=ses_id,\n",
    "    incl_STEPS=False,\n",
    "    incl_EPHYS=False,\n",
    "    incl_EMA=True,\n",
    "    incl_ACC=True,\n",
    "    day_selection=dev_day_selection\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check available EMAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_home_compl.plot_EMA_completion_perSession(home_dat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract features from Acc-Windows aligned to EMAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(data_handling)\n",
    "importlib.reload(ft_extr)\n",
    "\n",
    "\n",
    "xy_dict = {}\n",
    "\n",
    "iter_settings = {\n",
    "    'nosm_allwindow':[False, True, True],\n",
    "    'sm_merged': [True, False, False],\n",
    "    'sm_single': [True, False, True]}\n",
    "\n",
    "\n",
    "# EXTR_SM, SM_SINGLE = False, False\n",
    "# EXTR_FULLWIN = True\n",
    "\n",
    "for key, [EXTR_SM, EXTR_FULLWIN, SM_SINGLE] in iter_settings.items():\n",
    "\n",
    "    xy_dict[key] = ft_extr.get_features_per_session(\n",
    "        home_dat=home_dat,\n",
    "        sub_id=sub_id,\n",
    "        ses_id=ses_id,\n",
    "        LOAD_SAVE_FEATS = True,\n",
    "        # define how features should be extracted\n",
    "        EXTRACT_FT_FROM_SMs = EXTR_SM,\n",
    "        EXTRACT_FT_FULL_WIN = EXTR_FULLWIN,\n",
    "        ACC_FEATS_on_SINGLE_MOVES = SM_SINGLE,\n",
    "        STORE_SUBMOVES = False,\n",
    "        # plotting settings\n",
    "        SAVE_PLOT = False,\n",
    "        SHOW_PLOT = False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[f'{k}: {v.shape}, column-names: {list(v.keys())}' for k, v in xy_dict.items()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### feature extractions for full days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(acc_prep)\n",
    "importlib.reload(load_watch)\n",
    "importlib.reload(ft_extr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc_dayClass = acc_prep.get_accel_day(dat=home_dat, day_string='2025-07-14',)\n",
    "\n",
    "# day_dict_lists = acc_prep.get_day_ALL_AccWindows(\n",
    "#     subSesClass=home_dat, str_day='2025-07-12',\n",
    "#     START_HOUR=8, END_HOUR=22,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_settings = {\n",
    "    'nosm_allwindow': {\n",
    "        \"EXTRACT_FT_FROM_SMs\": False,\n",
    "        \"EXTRACT_FT_FULL_WIN\": True,\n",
    "        \"ACC_FEATS_on_SINGLE_MOVES\": False\n",
    "    },\n",
    "    'sm_merged': {\n",
    "        \"EXTRACT_FT_FROM_SMs\": True,\n",
    "        \"EXTRACT_FT_FULL_WIN\": False,\n",
    "        \"ACC_FEATS_on_SINGLE_MOVES\": False\n",
    "    },\n",
    "    'sm_single': {\n",
    "        \"EXTRACT_FT_FROM_SMs\": True,\n",
    "        \"EXTRACT_FT_FULL_WIN\": False,\n",
    "        \"ACC_FEATS_on_SINGLE_MOVES\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "ft_set_sel = 'sm_single'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "temp_fts = ft_extr.get_features_per_session(\n",
    "    ONLY_EMA_WINDOWS=False,\n",
    "    home_dat=home_dat,\n",
    "    sub_id=sub_id,\n",
    "    ses_id=ses_id,\n",
    "    LOAD_SAVE_FEATS = True,\n",
    "    # define how features should be extracted\n",
    "    STORE_SUBMOVES = False,\n",
    "    # plotting settings\n",
    "    SAVE_PLOT = False,\n",
    "    SHOW_PLOT = False,\n",
    "    **iter_settings[ft_set_sel],\n",
    ")\n",
    "\n",
    "# merge into one df\n",
    "ft_full_ses = pd.DataFrame()\n",
    "\n",
    "for d, f in temp_fts.items():\n",
    "    ft_full_ses = pd.concat([ft_full_ses, f]).reset_index(drop=True)\n",
    "\n",
    "ft_full_ses = ft_full_ses.set_index(keys=\"timestamp\")\n",
    "\n",
    "# check missings\n",
    "# np.sum(pd.isna(ft_full_ses), axis=0), ft_full_ses.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### identify and remove rows without submovements for submovement ft sets\n",
    "if ft_set_sel.startswith('sm'):\n",
    "    # select out windows without submoves\n",
    "    NOMOVE_CRIT_1 = pd.isna(ft_full_ses['rms_acc_SMcfvar'])\n",
    "    NOMOVE_CRIT_2 = ft_full_ses['rms_acc_SMmean'] == 0.0\n",
    "    nomove_idx = np.where(np.logical_and(NOMOVE_CRIT_1, NOMOVE_CRIT_2))[0]\n",
    "    move_idx = np.where(~np.logical_and(NOMOVE_CRIT_1, NOMOVE_CRIT_2))[0]\n",
    "    nomove_times = ft_full_ses.index[nomove_idx]\n",
    "    # only keep rows with submovements\n",
    "    ft_full_ses = ft_full_ses.iloc[move_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_full_ses.shape, len(nomove_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualise and check submovement velocity and PCAs\n",
    "- !! currently requires code parts that are internalised within ft_extr.get_features_per_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# for i_sm, tempwin in enumerate(sm_win_data[:30]):\n",
    "\n",
    "#     for att in ['pc1', 'pc2']:\n",
    "#         plt.plot(tempwin.timestamps, getattr(tempwin, att), label=att,\n",
    "#                 lw=3, alpha=.5,)\n",
    "\n",
    "#     plt.plot(tempwin.timestamps, tempwin.svm, label='svm',)\n",
    "#     plt.plot(tempwin.timestamps, tempwin.velocity.T,\n",
    "#             label=[f'velo_{a}' for a in 'xyz'],\n",
    "#             ls='--',)\n",
    "\n",
    "#     plt.title(f'submovement # {i_sm}')\n",
    "#     plt.legend()\n",
    "\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# plt.plot(tempwin.velocity.T)\n",
    "\n",
    "# pca = PCA(n_components=2)\n",
    "# projected = pca.fit_transform(tempwin.velocity.T)  # Shape: (N, 2)\n",
    "\n",
    "# pc1 = projected[:, 0]  # Primary direction\n",
    "# pc2 = projected[:, 1]  # Secondary direction\n",
    "\n",
    "# plt.plot(pc1, alpha=.3, lw=5,)\n",
    "# plt.plot(pc2, alpha=.3, lw=5,)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(np.abs(tempwin.velocity.T))\n",
    "\n",
    "# pca = PCA(n_components=2)\n",
    "# projected = pca.fit_transform(np.abs(tempwin.velocity.T))  # Shape: (N, 2)\n",
    "\n",
    "# pc1 = projected[:, 0]  # Primary direction\n",
    "# pc2 = projected[:, 1]  # Secondary direction\n",
    "\n",
    "# plt.plot(pc1, alpha=.3, lw=5,)\n",
    "# plt.plot(pc2, alpha=.3, lw=5,)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "visualise heartrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importlib.reload(load_watch)\n",
    "\n",
    "# hr_day_data = load_watch.get_source_heartrate_day(\n",
    "#     sub=home_dat.sub, ses=home_dat.ses, date='2025-08-13',\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t1 = windat.acc_times[0]\n",
    "# t2 = windat.acc_times[-1]\n",
    "\n",
    "# hr_sel = np.logical_and(hr_day_data['timestamp'] > t1,\n",
    "#                         hr_day_data['timestamp'] < t2)\n",
    "\n",
    "# hr_win = hr_day_data[hr_sel].reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "# hr_win"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(1, 1)\n",
    "\n",
    "# ax2 = ax.twinx()\n",
    "\n",
    "# ax.plot(windat.acc_times, windat.acc_svm)\n",
    "# ax2.plot(hr_win['timestamp'], hr_win[' HeartRate'], color='orangered',)\n",
    "\n",
    "# ax2.plot(hr_day_data['timestamp'], hr_day_data[' HeartRate'],\n",
    "#          color='orangered',)\n",
    "\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check of class-based ft extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(acc_fts)\n",
    "importlib.reload(ft_extr)\n",
    "\n",
    "for ft in FEATS_INCL:\n",
    "\n",
    "    value = getattr(sm_ft_class, f'run_{ft}')()  # extra brackets () for executing function\n",
    "\n",
    "    print(f'{ft}: {value}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in case later necessary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_datetime(t, T_RES_Sec = .1):\n",
    "    \"\"\"rounds on 0.1 sec\"\"\"\n",
    "\n",
    "    t_sec = t.microsecond / 1e6\n",
    "    t_sec_round = round(t_sec, abs(np.log10(T_RES_Sec)).astype(int))\n",
    "    micro = t_sec_round * 1e6\n",
    "\n",
    "    # add full second if rounding goes to 1e6 microseconds\n",
    "    if micro == 1e6:\n",
    "        t = t.replace(microsecond=0)\n",
    "        t += dt.timedelta(seconds=1)\n",
    "    # else replace rounded microseconds\n",
    "    else:\n",
    "        t = t.replace(microsecond=int(micro))\n",
    "\n",
    "    return t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate extracted Features\n",
    "\n",
    "- Hssayeni et al, Scientific Reports 2021\n",
    "    - strongest wrist-features: angular velocity, standard deviation, power of secondary frequency, power of 1â€“4 Hz band, and Shannon Entropy (r = 0.82  - r = 0.75)\n",
    "\n",
    "- from svm: classic features\n",
    "- include cross-corr between pc1 and pc2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate Features per window\n",
    "- distinction between extraciton over merged-svm or per-submovement is done in extraction code above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_values(\n",
    "    values, ZSCORE=True, LOG=True, return_kept_idx=False,\n",
    "    SET_Z_mean_std=None,\n",
    "):\n",
    "\n",
    "    # log transform    \n",
    "    if LOG:\n",
    "        # remove zeros and nans\n",
    "        if return_kept_idx:\n",
    "            kept_idx = np.where([~np.isnan(v) and v != 0.0 for v in values])[0]\n",
    "        values = [v for v in values if ~np.isnan(v) and v != 0.0]\n",
    "        values = np.log(values)\n",
    "        \n",
    "    # zscore\n",
    "    if ZSCORE:\n",
    "        if SET_Z_mean_std:\n",
    "            zmean, zstd = SET_Z_mean_std\n",
    "        else:\n",
    "            zmean, zstd = np.nanmean(values), np.nanstd(values)\n",
    "        values = (values - zmean) / zstd\n",
    "\n",
    "    \n",
    "    \n",
    "    if return_kept_idx:\n",
    "        return values, kept_idx\n",
    "    else:\n",
    "        return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MULTI_SESH = False\n",
    "EXCL_HR = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get array with shape n-windows, n-feats\n",
    "\n",
    "if MULTI_SESH:\n",
    "    ### MULTI-SESSION DATA\n",
    "    X = np.concat([X_ALL['ses01'], X_ALL['ses02']]).T\n",
    "    y = np.concat([y_ALL['ses01'], y_ALL['ses02']]).ravel()\n",
    "else:\n",
    "    ### SINGLE SESSION DATA\n",
    "    X = np.array([l for l in list(FEAT_STORE.values())])\n",
    "    # get array with length n-windows\n",
    "    y = np.array(Y_STORE['LID']).astype(float)\n",
    "\n",
    "print(X.shape)\n",
    "\n",
    "# WITHOUT HR BCS OF NANS\n",
    "if EXCL_HR:\n",
    "    hr_sel = ['hr_' in k for k in list(FEAT_STORE.keys())]\n",
    "    X = X[~np.array(hr_sel), :]\n",
    "    FTNAMES_KEPT = [n for n in list(FEAT_STORE.keys()) if 'hr_' not in n]\n",
    "else:\n",
    "    FTNAMES_KEPT = list(FEAT_STORE.keys())\n",
    "\n",
    "\n",
    "print(X.shape)\n",
    "\n",
    "\n",
    "for i_ft, (ft_X, ft_name) in enumerate(zip(X, FTNAMES_KEPT)):\n",
    "    # repeat y definition due to nan-dropping in normalization\n",
    "    y_iter = y.copy()\n",
    "\n",
    "    print(ft_X.shape, ft_name, len(y_iter))\n",
    "\n",
    "    ft_X, kept_idx = normalize_values(ft_X, ZSCORE=True, LOG=True,\n",
    "                                      return_kept_idx=True,\n",
    "                                      SET_Z_mean_std=None,)\n",
    "    print('kept:', len(kept_idx), 'total', len(y))\n",
    "    y_iter = y_iter[kept_idx]\n",
    "\n",
    "    fig, ax = plt.subplots(1,1, figsize=(8, 3))\n",
    "\n",
    "    box_values = {str(y_value): [] for y_value in np.arange(1, 10)}\n",
    "\n",
    "    for y_value in list(box_values.keys()):\n",
    "        box_sel = y_iter == float(y_value)\n",
    "        matching_values = ft_X[box_sel]\n",
    "        matching_values = matching_values[~np.isnan(matching_values)]  # get rid of NaNs\n",
    "        box_values[y_value].extend(matching_values)\n",
    "\n",
    "    ax.boxplot(box_values.values())\n",
    "\n",
    "    ax.set_xticklabels(list(box_values.keys()))\n",
    "\n",
    "    ax.set_ylabel(ft_name)\n",
    "    ax.set_title(ft_name)\n",
    "    ax.set_xlabel('EMA Dyskinesia severity (Likert points)')\n",
    "\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for c, m in zip(FEAT_STORE['sm_count'], FEAT_STORE['sm_duration_mean']):\n",
    "#     print(f\"# {c},\\t\\t{round(c * m / 6, 1)} segments,\\t\\tmean: {round(m, 1)} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check feature distributions\n",
    "- pre and post log-transform and z-scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get array with shape n-windows, n-feats\n",
    "# X = np.array([l for l in list(FEAT_STORE.values())])\n",
    "# # get array with length n-windows\n",
    "# y = np.array(Y_STORE['LID']).astype(float)\n",
    "\n",
    "ft_set = list(xy_dict.keys())[2]\n",
    "\n",
    "skip_feats = ['timestamp', ]\n",
    "\n",
    "for ftname, ftvalues in xy_dict[ft_set].items():\n",
    "\n",
    "    if ftname in skip_feats: continue\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(8, 3))\n",
    "\n",
    "    axes[0].hist(ftvalues)\n",
    "    axes[0].set_ylabel('count (n)')\n",
    "    axes[0].set_xlabel('raw values')\n",
    "\n",
    "    # z-score and take log values\n",
    "    # try:\n",
    "    ftvalues = normalize_values(ftvalues, ZSCORE=True, LOG=True)\n",
    "    # except:\n",
    "    #     print(f'\\n\\n{ftname} FAILED:')\n",
    "    #     print(ftvalues)\n",
    "    axes[1].hist(ftvalues)\n",
    "    axes[1].set_ylabel('count (n)')\n",
    "    axes[1].set_xlabel('log. z-scored values')\n",
    "\n",
    "    plt.suptitle(ftname)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### make daycurves for full day features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check leftovers NaNs\n",
    "np.sum(pd.isna(ft_full_ses), axis=0)\n",
    "\n",
    "# drop heartrate columns\n",
    "ft_full_ses = ft_full_ses.drop(columns=[k for k in ft_full_ses.columns if k.startswith('hr')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dayminutes(t):\n",
    "\n",
    "    if type(t) == str:\n",
    "        t = dt.datetime.strptime(t[:16], \"%Y-%m-%d %H:%M\")\n",
    "\n",
    "    minutes = t.minute\n",
    "    minutes += (t.hour * 60)\n",
    "\n",
    "    return minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_daily_minutes_mask(HR_START=8, HR_END=22, WIN_LEN_minutes=15):\n",
    "\n",
    "    # get day time raster (in minutes into day)\n",
    "    t0 = dt.datetime.strptime(str(HR_START), \"%H\")\n",
    "    n_wins = (HR_END - HR_START) * (60 / WIN_LEN_minutes)\n",
    "    mask_dtimes = [t0 + dt.timedelta(minutes=int(WIN_LEN_minutes * i))\n",
    "                   for i in np.arange(n_wins)]\n",
    "    mask_minutes = [get_dayminutes(t) for t in mask_dtimes]\n",
    "\n",
    "    return mask_minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ft_daily_mean(\n",
    "    ft_values, ft_times,\n",
    "    PLOT_SAMPLE_DISTRIBUTION=False,\n",
    "):\n",
    "    \n",
    "    mask_minutes = get_daily_minutes_mask()\n",
    "    mask_dict = {m: [] for m in mask_minutes}\n",
    "\n",
    "    for i, v in enumerate(ft_values):\n",
    "        # get daily minute of timestamp (df-index)\n",
    "        t = get_dayminutes(ft_times[i])\n",
    "        # add ft value to list of corresponding daily minute\n",
    "        try:\n",
    "            mask_dict[t].append(v)\n",
    "        except KeyError:\n",
    "            # daily minutes not correct, find closest\n",
    "            i_alt = np.argmin(abs(np.array(mask_minutes) - t))\n",
    "            min_alt = mask_minutes[i_alt]\n",
    "            mask_dict[min_alt].append(v)\n",
    "    \n",
    "    daily_minutes = list(mask_dict.keys())\n",
    "    daily_mean = np.array([np.nanmean(l) for l in mask_dict.values()])\n",
    "    daily_std = np.array([np.nanstd(l) for l in mask_dict.values()])\n",
    "\n",
    "    ### plot distribution of samples on daily-minutes-raster\n",
    "    if PLOT_SAMPLE_DISTRIBUTION:\n",
    "        for k, v in mask_dict.items():\n",
    "            print(f'{k} min, {k / 60} h: {len(v)} samples')\n",
    "\n",
    "        samples_at_minutes = [len(l) for l in mask_dict.values()]\n",
    "        print(samples_at_minutes)\n",
    "        fig,ax = plt.subplots(1,1, figsize=(8, 3))\n",
    "        ax.bar(x=daily_minutes, height=samples_at_minutes,\n",
    "               width=12, color='olivedrab', alpha=.3,)\n",
    "        # ax.plot(daily_minutes, [len(l) for l in mask_dict.values()])\n",
    "        ax.set_ylabel('n samples (count)')\n",
    "        ax.set_xlabel('Time at Day (hours)')\n",
    "        ax.set_xticks(daily_minutes[::8])\n",
    "        ax.set_xticklabels((np.array(daily_minutes[::8])/60).astype(int),)\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    return daily_minutes, daily_mean, daily_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_daily_ft_mean(\n",
    "    daily_minutes, daily_mean, daily_std, ft_name,\n",
    "    use_ax = None,\n",
    "):\n",
    "\n",
    "    plot_color = 'olivedrab'  # orchid, blue, olivedrab\n",
    "    xtick_hop = 8\n",
    "\n",
    "    if not use_ax:\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(8, 3))\n",
    "        RETURN_AX = False\n",
    "    else:\n",
    "        ax = use_ax\n",
    "        RETURN_AX = True\n",
    "\n",
    "    ax.plot(daily_minutes, daily_mean, color=plot_color, lw=3,)\n",
    "    ax.fill_between(daily_minutes, y1=daily_mean - daily_std,\n",
    "                    y2=daily_mean + daily_std, alpha=.3,\n",
    "                    color=plot_color,)\n",
    "\n",
    "    ax.set_xticks(daily_minutes[::xtick_hop])\n",
    "    ax.set_xticklabels((np.array(daily_minutes[::xtick_hop])/60).astype(int),)\n",
    "    ax.set_xlabel('Time at Day (hours)')\n",
    "\n",
    "    ax.set_ylabel(ft_name)\n",
    "\n",
    "    if RETURN_AX:\n",
    "        return ax\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FT_SEL_LIST = [\n",
    "    'sm_count',\n",
    "    'rms_acc_SMmean',\n",
    "    'pow_4_7_ratio_SMmean',\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(len(FT_SEL_LIST), 1,\n",
    "                         figsize=(8, 3*len(FT_SEL_LIST)))\n",
    "\n",
    "for i_ax, FT_SEL in enumerate(FT_SEL_LIST):\n",
    "    (\n",
    "        daily_minutes, daily_mean, daily_std\n",
    "    ) = get_ft_daily_mean(ft_full_ses[FT_SEL], ft_full_ses.index,)\n",
    "\n",
    "    plot_daily_ft_mean(\n",
    "        daily_minutes, daily_mean, daily_std, FT_SEL, use_ax=axes[i_ax]\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict EMAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, Lasso\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, r2_score,\n",
    "    roc_auc_score, balanced_accuracy_score\n",
    ")\n",
    "from scipy.stats import f as stats_f\n",
    "from sklearn.feature_selection import f_regression, r_regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_f_stat(y_pred, y_true, n_feats):\n",
    "    # F-statistic for model usability\n",
    "    \n",
    "    # Sum of squares\n",
    "    SSR = np.sum((y_pred - np.mean(y_true)) ** 2)   # Regression\n",
    "    SSE = np.sum((y_true - y_pred) ** 2)   # Error\n",
    "    SST = np.sum((y_true - np.mean(y_true)) ** 2)   # Total\n",
    "\n",
    "    # Degrees of freedom\n",
    "    df_reg = n_feats\n",
    "    df_err = len(y_true) - (n_feats + 1)  # +1 for coeff next to betas of features\n",
    "\n",
    "    # Mean squares\n",
    "    MSR = SSR / df_reg if df_reg > 0 else np.nan\n",
    "    MSE = SSE / df_err if df_err > 0 else np.nan\n",
    "    F = MSR / MSE\n",
    "    # p-value for the observed F\n",
    "    f_pval = 1 - stats_f.cdf(F, df_reg, df_err)\n",
    "\n",
    "    return F, f_pval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cross-validation n=1 concept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define X and y\n",
    "- clear NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_X_y(\n",
    "    X, y, REMOVE_ZERO_SUBMOVES=False, verbose=False,\n",
    "    RETURN_EXCLUDED_NAN_BOOL=False,\n",
    "):\n",
    "    \n",
    "    nan_rows = np.any(np.isnan(X), axis=1)\n",
    "\n",
    "    if REMOVE_ZERO_SUBMOVES:\n",
    "        zero_rows = np.any(X == 0.0, axis=1)\n",
    "        nan_rows = np.logical_or(nan_rows, zero_rows)\n",
    "\n",
    "    if verbose: print(f'removed window-rows bcs of NaNs: n={sum(nan_rows)}')\n",
    "    X = X[~nan_rows]\n",
    "\n",
    "    # double check nan removing\n",
    "    check_nan_rows = np.any(np.isnan(X), axis=1)\n",
    "    if verbose: print(f'after removing n-nanrows: n={sum(check_nan_rows)}')\n",
    "    \n",
    "    # z-score and log\n",
    "    for i_col in np.arange(X.shape[1]):\n",
    "        x_temp = X[:, i_col]\n",
    "        x_temp = normalize_values(x_temp, ZSCORE=True, LOG=True,)\n",
    "        X[:, i_col] = x_temp\n",
    "\n",
    "    # get array with length n-windows\n",
    "    if len(y) > 0: y = y[~nan_rows]\n",
    "    else: y = None\n",
    "\n",
    "    if not RETURN_EXCLUDED_NAN_BOOL: return X, y\n",
    "    else: return X, y, nan_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXCL_HR = True\n",
    "ZSCORE_Y = False\n",
    "\n",
    "# # get array with shape n-windows, n-feats\n",
    "# X = np.array([l for l in list(FEAT_STORE.values())]).T\n",
    "# # get array with length n-windows\n",
    "# y = np.array(Y_STORE['LID']).astype(float).reshape(-1, 1)\n",
    "\n",
    "ft_set = list(xy_dict.keys())[2]\n",
    "print(f'\\fFeature set chosen: {ft_set}')\n",
    "\n",
    "keys_excl_feats = [\n",
    "    'timestamp', 'LID', 'tremor', 'overall_move', 'move_hands', 'wellbeing'\n",
    "]\n",
    "FTS_INCL = [k for k in list(xy_dict[ft_set])\n",
    "            if (k not in keys_excl_feats) and ('hr' not in k)]\n",
    "EMA_Y = 'LID'\n",
    "\n",
    "if MULTI_SESH:\n",
    "    ### MULTI-SESSION DATA\n",
    "    X = np.concat([X_ALL['ses01'], X_ALL['ses02']]).T\n",
    "    y = np.concat([y_ALL['ses01'], y_ALL['ses02']]).ravel()\n",
    "else:\n",
    "    ### SINGLE SESSION DATA\n",
    "    X = xy_dict[ft_set][FTS_INCL].values.copy()\n",
    "    y = xy_dict[ft_set][EMA_Y].values.astype(float)\n",
    "    X, y = prep_X_y(X, y)  # delete nans, log, z-score\n",
    "\n",
    "\n",
    "# # WITHOUT HR BCS OF NANS\n",
    "# if EXCL_HR:\n",
    "#     hr_sel = ['hr_' in k for k in list(FEAT_STORE.keys())]\n",
    "#     X = X[~np.array(hr_sel), :]\n",
    "#     FTNAMES_KEPT = [n for n in list(FEAT_STORE.keys()) if 'hr_' not in n]\n",
    "# else:\n",
    "#     FTNAMES_KEPT = list(FEAT_STORE.keys())\n",
    "\n",
    "if ZSCORE_Y:\n",
    "    y = ((y - np.mean(y)) / np.std(y))\n",
    "    # y = (y - Z_y_mean) / Z_y_std\n",
    "\n",
    "\n",
    "# plt.plot(y)\n",
    "# plt.show()\n",
    "# print(X.shape, y.shape)\n",
    "\n",
    "# # WITHOUT HR BCS OF NANS\n",
    "# if EXCL_HR:\n",
    "#     hr_sel = ['hr_' in k for k in list(FEAT_STORE.keys())]\n",
    "#     X = X[:, ~np.array(hr_sel)]\n",
    "# print(X.shape, y.shape)\n",
    "\n",
    "# check where nans are, TODO during creation\n",
    "# list(compress(list(FEAT_STORE.keys()), np.any(np.isnan(X), axis=1)))\n",
    "\n",
    "# if X.shape[0] > X.shape[1]:\n",
    "#     nonnan_sel = ~np.any(np.isnan(X), axis=1)\n",
    "#     y = y[nonnan_sel]\n",
    "#     X = X[nonnan_sel, :]\n",
    "# else:\n",
    "#     nonnan_sel = ~np.any(np.isnan(X), axis=0)\n",
    "#     X = X[:, nonnan_sel]\n",
    "#     y = y[nonnan_sel]\n",
    "\n",
    "\n",
    "print(X.shape, y.shape, len(FTS_INCL))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### compare different feature sets (full-window vs single-movements extracted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotting import plot_home_preds\n",
    "from utils import pred_nat_ema "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ft_set in list(xy_dict.keys()):\n",
    "\n",
    "      print(f'\\n{\"#\" * 5}\\nFeature set chosen: {ft_set}')\n",
    "\n",
    "      FTS_INCL = [k for k in list(xy_dict[ft_set])\n",
    "                  if (k not in keys_excl_feats) and ('hr' not in k)]\n",
    "      \n",
    "      print(FTS_INCL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(plot_home_preds)\n",
    "importlib.reload(pred_nat_ema)\n",
    "\n",
    "\n",
    "### SELECT FEATURE SET for prediction\n",
    "\n",
    "ft_set = list(xy_dict.keys())[2]\n",
    "# print(f'\\fFeature set chosen: {ft_set}')\n",
    "\n",
    "keys_excl_feats = [\n",
    "    'timestamp', 'LID', 'tremor', 'overall_move', 'move_hands', 'wellbeing'\n",
    "]\n",
    "EMA_Y = 'LID'\n",
    "\n",
    "\n",
    "for ft_set in xy_dict.keys():\n",
    "\n",
    "      if ft_set != 'sm_single': continue\n",
    "\n",
    "      if ft_set.startswith('sm'): EXCL_ZEROS = True\n",
    "      else: EXCL_ZEROS = False\n",
    "\n",
    "      print(f'\\n{\"#\" * 50}\\nFeature set chosen: {ft_set}')\n",
    "\n",
    "      FTS_INCL = [k for k in list(xy_dict[ft_set])\n",
    "                  if (k not in keys_excl_feats) and ('hr' not in k)]\n",
    "\n",
    "      X = xy_dict[ft_set][FTS_INCL].values.copy()\n",
    "      y = xy_dict[ft_set][EMA_Y].values.copy().astype(float)\n",
    "      # get day array for crossval leave day out\n",
    "      days = xy_dict[ft_set]['timestamp']  # get timestamps per sample\n",
    "      days = [d.split(' ')[0] for d in days]  # only take day string per timestamp\n",
    "      day_codes = [np.where(np.unique(days) == d)[0][0] for d in days]  # convert strings into codes per day\n",
    "      \n",
    "      # TODO: include day in prep X y \n",
    "      X, y, excluded_bool = prep_X_y(\n",
    "            X, y, REMOVE_ZERO_SUBMOVES=EXCL_ZEROS, verbose=False,\n",
    "            RETURN_EXCLUDED_NAN_BOOL=True,\n",
    "      )  # delete nans, log, z-score\n",
    "      if sum(excluded_bool) > 0:\n",
    "            day_codes = np.array(day_codes)[~excluded_bool]\n",
    "            assert len(day_codes) == X.shape[0], 'incorrect day code length vs X'\n",
    "\n",
    "      ### predict\n",
    "      PERMS = False\n",
    "      N_PERMS = 1000\n",
    "      TO_PLOT = True\n",
    "\n",
    "      CLS_METHOD = 'lasso'\n",
    "      n_fold_cv = 3\n",
    "\n",
    "      if not PERMS:\n",
    "            y_pred_total = pred_nat_ema.classif_home_ema(\n",
    "                  X=X, y=y, PERMS=PERMS, N_PERMS=N_PERMS, n_fold_cv=n_fold_cv,\n",
    "                  CLS_METHOD=CLS_METHOD, ZSCORE_Y=ZSCORE_Y, verbose=False,\n",
    "                  day_codes=day_codes, leave_day_out_cv=True,\n",
    "            )\n",
    "\n",
    "      else:\n",
    "            print('\\n\\n ############\\nADD PERMUTATION FLOW')\n",
    "\n",
    "\n",
    "      #### PLOTTING\n",
    "      if TO_PLOT:\n",
    "            if MULTI_SESH:\n",
    "                  FIGNAME = f'LidPred_{sub_id}_ses0102_{CLS_METHOD}_scatter_n{len(y)}'\n",
    "            else:\n",
    "                  FIGNAME = f'LidPred_{sub_id}_{ses_id}_{CLS_METHOD}_scatter_n{len(y)}'\n",
    "            if EXCL_HR: FIGNAME += '_exclHR'\n",
    "            else: FIGNAME += '_inclHR'\n",
    "\n",
    "            plot_home_preds.scatter_preds(y=y, y_pred_total=y_pred_total,\n",
    "                                          ZSCORE_Y=ZSCORE_Y, show=TO_PLOT,)\n",
    "\n",
    "\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train on EMA, test on whole session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLS_METHOD = 'lasso'\n",
    "\n",
    "CLS_LIB = {\n",
    "    'linreg': LinearRegression(),\n",
    "    'lda': LDA(),  # only applicable on categorical or binary values\n",
    "    'lasso': Lasso(alpha=0.3)\n",
    "}\n",
    "\n",
    "\n",
    "FTS_INCL_train = [\n",
    "    k for k in list(xy_dict[ft_set])\n",
    "    if (k not in keys_excl_feats) and ('hr' not in k)\n",
    "]\n",
    "\n",
    "\n",
    "### define training X, y\n",
    "X_ema_train = xy_dict[ft_set][FTS_INCL_train].values.copy()\n",
    "y_ema_train = xy_dict[ft_set][EMA_Y].values.copy().astype(float)\n",
    "\n",
    "X_ema_train, y_ema_train = prep_X_y(\n",
    "    X_ema_train, y_ema_train,\n",
    "    REMOVE_ZERO_SUBMOVES=EXCL_ZEROS, verbose=False,\n",
    ")  # delete nans, log, z-score\n",
    "\n",
    "# train model\n",
    "clf = CLS_LIB[CLS_METHOD]\n",
    "clf.fit(X_ema_train, y_ema_train)\n",
    "\n",
    "### define test X\n",
    "X_all_test = ft_full_ses.values.astype(float)\n",
    "test_stamps = [\n",
    "    dt.datetime.strptime(t[:19], \"%Y-%m-%d %H:%M:%S\")\n",
    "    for t in ft_full_ses.index.values\n",
    "]\n",
    "X_all_test, _ = prep_X_y(\n",
    "    X_all_test, y=[],\n",
    "    REMOVE_ZERO_SUBMOVES=EXCL_ZEROS, verbose=False,\n",
    ")  # delete nans, log, z-score\n",
    "\n",
    "# compare features\n",
    "assert list(ft_full_ses.keys()) == FTS_INCL, 'different feature sets'\n",
    "\n",
    "test_pred = clf.predict(X_all_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_stamps, test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### daily plot\n",
    "(\n",
    "    daily_minutes, daily_mean, daily_std\n",
    ") = get_ft_daily_mean(test_pred, test_stamps,)\n",
    "\n",
    "plot_daily_ft_mean(\n",
    "    daily_minutes, daily_mean, daily_std, 'LID prediction', #use_ax=axes[i_ax]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perm test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N_PERMS = 1000\n",
    "\n",
    "# # TEST_SEL = daycode_arr > 20  # takes circa .33\n",
    "\n",
    "# # X_train = X_arr[~TEST_SEL, :]\n",
    "# # y_train = y_arr[~TEST_SEL]\n",
    "\n",
    "# # # test cohort\n",
    "# # X_test = X_arr[TEST_SEL, :]\n",
    "# # y_true = y_arr[TEST_SEL]\n",
    "\n",
    "# model = LDA()\n",
    "\n",
    "# # Run prediction\n",
    "# perm_mets ={'F': [], 'R': []}\n",
    "\n",
    "# y_true = y_arr.astype(int)\n",
    "\n",
    "# np.random.seed(27)\n",
    "\n",
    "# for i_perm in np.arange(N_PERMS):\n",
    "\n",
    "#     y_perm_pred = np.array([np.nan] * len(y_arr))\n",
    "\n",
    "#     for day in np.unique(daycode_arr):\n",
    "\n",
    "#         TEST_SEL = daycode_arr == day\n",
    "#         X_train = X_arr[~TEST_SEL, :]\n",
    "#         y_train = y_arr.astype(int)[~TEST_SEL]\n",
    "#         np.random.shuffle(y_train)   \n",
    "\n",
    "#         # test cohort\n",
    "#         X_test = X_arr[TEST_SEL, :]\n",
    "\n",
    "#         model = models[CLSF]\n",
    "#         model.fit(X_train, y_train)\n",
    "\n",
    "#         y_perm_pred[TEST_SEL] = model.predict(X_test)\n",
    "\n",
    "#     # y_train = y_arr[~TEST_SEL]\n",
    "#     # np.random.shuffle(y_train)   \n",
    "#     # model.fit(X_train, y_train)\n",
    "\n",
    "#     # y_perm_pred = model.predict(X_test)\n",
    "#     # # np.random.shuffle(y_perm_pred)\n",
    "#     y_perm_pred = np.array([np.round(v) for v in y_perm_pred])\n",
    "\n",
    "#     F, f_pvalue = get_f_stat(y_pred=y_perm_pred, y_true=y_true,\n",
    "#                              n_feats=X_test.shape[1])\n",
    "#     prs_stat, _ = pearsonr(y_perm_pred, y_true)\n",
    "#     perm_mets['F'].append(F)\n",
    "#     perm_mets['R'].append(prs_stat)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MULTI_SESH:\n",
    "    FIGNAME = f'LidPred_{sub_id}_SESS0102_{CLS_METHOD}_PermSign_n{N_PERMS}'\n",
    "else:\n",
    "    FIGNAME = f'LidPred_{sub_id}_{ses_id}_{CLS_METHOD}_PermSign_n{N_PERMS}'\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8, 3))\n",
    "\n",
    "# pred_mets = {'F': pred_F, 'R': pred_corrcoef}\n",
    "\n",
    "for i_ax, metr in enumerate(['F', 'R']):\n",
    "\n",
    "    axes[i_ax].hist(perm_results[metr], color='gray', alpha=.5,)\n",
    "    axes[i_ax].axvline(np.percentile(perm_results[metr], 97.5),\n",
    "                       color='orange', alpha=.8, lw=3,\n",
    "                       label=f'alpha=0.025\\n(n-perm: {N_PERMS})',)\n",
    "    \n",
    "    axes[i_ax].axvline(perm_true_results[metr],\n",
    "                       color='purple', alpha=.5, lw=1,\n",
    "                       label='prediction',)\n",
    "    \n",
    "    p_calc = sum(perm_true_results[metr] < perm_results[metr]) / len(perm_results[metr])\n",
    "    print(f'metric {metr}: p = {np.round(p_calc, 3)}')\n",
    "\n",
    "    axes[i_ax].set_xlabel(f'{metr} score', size=14,)\n",
    "\n",
    "    axes[i_ax].set_ylabel('count (n)', size=14)\n",
    "\n",
    "    axes[i_ax].tick_params(axis='both', size=14, labelsize=14,)\n",
    "    axes[i_ax].spines[['right', 'top']].set_visible(False)\n",
    "\n",
    "axes[1].legend(frameon=False, fontsize=14,\n",
    "               bbox_to_anchor=(.95, .5), loc='center left')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# plt.savefig(os.path.join(load_utils.get_onedrive_path('figures'),\n",
    "#              'proof_kin_pred', FIGNAME),\n",
    "#              dpi=300, facecolor='w',)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "home",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
